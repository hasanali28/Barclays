{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random forest model is a collection of decision tree models that are combined together to make predictions. When you make a random forest, you have to specify the number of decision trees you want to use to make the model. The random forest algorithm then takes random samples of observations from your training data and builds a decision tree model for each sample. The random samples are typically drawn with replacement, meaning the same observation can be drawn multiple times. The end result is a bunch of decision trees that are created with different groups of data records drawn from the original training data.\n",
    "\n",
    "The decision trees in a random forest model are a little different than the standard decision trees we made last time. Instead of growing trees where every single explanatory variable can potentially be used to make a branch at any level in the tree, random forests limit the variables that can be used to make a split in the decision tree to some random subset of the explanatory variables. Limiting the splits in this fashion helps avoid the pitfall of always splitting on the same variables and helps random forests create a wider variety of trees to reduce overfitting.\n",
    "\n",
    "Random forests are an example of an ensemble model: a model composed of some combination of several different underlying models. Ensemble models often yields better results than single models because different models may detect different patterns in the data and combining models tends to dull the tendency that complex single models have to overfit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests on the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's sklearn package offers a random forest model that works much like the decision tree model we used last time. Let's use it to train a random forest model on the Titanic training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare Titanic data\n",
    "\n",
    "titanic_train = pd.read_csv(\"data/titanic/train.csv\")    # Read the data\n",
    "\n",
    "# Impute median Age for NA Age values\n",
    "new_age_var = np.where(titanic_train[\"Age\"].isnull(), # Logical check\n",
    "                       28,                       # Value if check is true\n",
    "                       titanic_train[\"Age\"])     # Value if check is false\n",
    "\n",
    "titanic_train[\"Age\"] = new_age_var "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB accuracy: \n",
      "0.8226711560044894\n"
     ]
    }
   ],
   "source": [
    "# Set the seed\n",
    "np.random.seed(12)\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Convert some variables to numeric\n",
    "titanic_train[\"Sex\"] = label_encoder.fit_transform(titanic_train[\"Sex\"])\n",
    "titanic_train[\"Embarked\"] = label_encoder.fit_transform(titanic_train[\"Embarked\"].astype(str))\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, # Number of trees\n",
    "                                  max_features=2,    # Num features considered\n",
    "                                  oob_score=True)    # Use OOB scoring*\n",
    "\n",
    "features = [\"Sex\",\"Pclass\",\"SibSp\",\"Embarked\",\"Age\",\"Fare\"]\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X=titanic_train[features],\n",
    "             y=titanic_train[\"Survived\"])\n",
    "\n",
    "print(\"OOB accuracy: \")\n",
    "print(rf_model.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since random forest models invovle building trees from random subsets or \"bags\" of data, model performance can be estimated by making predictions on the out-of-bag (OOB) samples instead of using cross validation. You can use cross validation on random forests, but OOB validation already provides a good estimate of performance and building several random forest models to conduct K-fold cross validation with random forest models can be computationally expensive.\n",
    "\n",
    "The random forest classifier assigns an importance value to each feature used in training. Features with higher improtance were more influential in creating the model, indiciating a stronger association with the response variable. Let's check the feature importances for our random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex 0.2681626132625183\n",
      "Pclass 0.08797315431120382\n",
      "SibSp 0.05125290319148758\n",
      "Embarked 0.03186684508628095\n",
      "Age 0.2730887800512412\n",
      "Fare 0.2876557040972684\n"
     ]
    }
   ],
   "source": [
    "for feature, imp in zip(features, rf_model.feature_importances_):\n",
    "    print(feature, imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance can help identify useful features and elimiante features that don't contribute much to the model.\n",
    "\n",
    "As a final exercise, let's use the random forest model to make predictions on the titanic test set and submit them to Kaggle to see how our actual generalization performance compares to the OOB estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and prepare test data\n",
    "titanic_test = pd.read_csv(\"data/titanic/test.csv\")    # Read the data\n",
    "\n",
    "# Impute median Age for NA Age values\n",
    "new_age_var = np.where(titanic_test[\"Age\"].isnull(),\n",
    "                       28,                      \n",
    "                       titanic_test[\"Age\"])      \n",
    "\n",
    "titanic_test[\"Age\"] = new_age_var \n",
    "\n",
    "# Convert some variable to numeric\n",
    "titanic_test[\"Sex\"] = label_encoder.fit_transform(titanic_test[\"Sex\"])\n",
    "titanic_test[\"Embarked\"] = label_encoder.fit_transform(titanic_test[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test set predictions\n",
    "test_preds = rf_model.predict(X= titanic_test[features].fillna(method='ffill'))\n",
    "\n",
    "# Create a submission for Kaggle\n",
    "submission = pd.DataFrame({\"PassengerId\":titanic_test[\"PassengerId\"],\n",
    "                           \"Survived\":test_preds})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv(\"tutorial_randomForest_submission.csv\", \n",
    "                  index=False)        # Do not save index values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon submission the random forest model achieves an accuracy score of 0.75120, which is actually worse than the decision tree model and even the simple gender-based model. What gives? Is the model overfitting the training data? Did we choose bad variables and model parameters? Or perhaps our simplistic imputation of filling in missing age data using median ages is hurting our accuracy. Data analyses and predictive models often don't turn out how you expect, but even a \"bad\" result can be valuable as it can give you more insight into your problem. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
